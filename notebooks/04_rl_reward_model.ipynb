{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ac6b160-5413-4b41-ab93-485be62f2c13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting textstat\n",
      "  Downloading textstat-0.7.7-py3-none-any.whl (175 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 175 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from textstat) (58.0.4)\n",
      "Collecting pyphen\n",
      "  Downloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.1 MB 22.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cmudict\n",
      "  Downloading cmudict-1.0.33-py3-none-any.whl (939 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 939 kB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-resources>=5 in /Users/aayushbhatia/Library/Python/3.9/lib/python/site-packages (from cmudict->textstat) (6.5.2)\n",
      "Requirement already satisfied: importlib-metadata>=5 in /Users/aayushbhatia/Library/Python/3.9/lib/python/site-packages (from cmudict->textstat) (8.6.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/aayushbhatia/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=5->cmudict->textstat) (3.21.0)\n",
      "Installing collected packages: pyphen, cmudict, textstat\n",
      "Successfully installed cmudict-1.0.33 pyphen-0.17.2 textstat-0.7.7\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install textstat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf306263-db40-4734-a35e-66a40beaaea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Reward Scores:\n",
      "{\n",
      "  \"length_score\": 0.0,\n",
      "  \"readability_score\": 0.0,\n",
      "  \"human_like_score\": 0.5,\n",
      "  \"final_reward\": 0.15\n",
      "}\n",
      "âœ… Reward saved at: ../assets/rewards/human_edit_latest.json\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“Œ Step 1: Imports\n",
    "import os\n",
    "import json\n",
    "from groq import Groq\n",
    "from textstat import flesch_reading_ease\n",
    "\n",
    "# ðŸ“Œ Step 2: GROQ API setup\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# ðŸ“Œ Step 1: Imports\n",
    "import datetime\n",
    "from groq import Groq\n",
    "\n",
    "# ðŸ“Œ Step 2: Setup GROQ API\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# ðŸ“Œ Step 3: Reward calculation function\n",
    "def calculate_reward(original_text, new_text):\n",
    "    # Length similarity\n",
    "    orig_len = len(original_text)\n",
    "    new_len = len(new_text)\n",
    "    length_score = 1 - abs(orig_len - new_len) / max(orig_len, new_len)\n",
    "\n",
    "    # Readability score\n",
    "    readability = flesch_reading_ease(new_text)\n",
    "    readability_score = min(readability / 100, 1)\n",
    "\n",
    "    # Human-likeness using LLM\n",
    "    prompt = f\"Rate how human-like and coherent the following passage is, on a scale of 1 to 10:\\n\\n---\\n{new_text}\\n---\\n\\nOnly reply with the number:\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama3-8b-8192\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.8,\n",
    "            max_tokens=5\n",
    "        )\n",
    "        human_like_score = float(response.choices[0].message.content.strip()) / 10\n",
    "    except:\n",
    "        human_like_score = 0.5\n",
    "\n",
    "    # Final reward score\n",
    "    final = (\n",
    "        0.4 * length_score +\n",
    "        0.3 * readability_score +\n",
    "        0.3 * human_like_score\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"length_score\": round(length_score, 3),\n",
    "        \"readability_score\": round(readability_score, 3),\n",
    "        \"human_like_score\": round(human_like_score, 3),\n",
    "        \"final_reward\": round(final, 3)\n",
    "    }\n",
    "\n",
    "# ðŸ“Œ Step 4: Correct paths as per structure\n",
    "original_path = \"../assets/raw_text/chapter1.txt\"\n",
    "version_path = \"../assets/human_edits/human_edit_latest.txt\"  # You can change this\n",
    "\n",
    "# ðŸ“Œ Step 5: Load both files\n",
    "with open(original_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    original_text = f.read()\n",
    "\n",
    "with open(version_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    new_text = f.read()\n",
    "\n",
    "# ðŸ“Œ Step 6: Calculate reward\n",
    "reward = calculate_reward(original_text, new_text)\n",
    "\n",
    "print(\"ðŸ“Š Reward Scores:\")\n",
    "print(json.dumps(reward, indent=2))\n",
    "\n",
    "# ðŸ“Œ Step 7: Save to JSON\n",
    "os.makedirs(\"../assets/rewards\", exist_ok=True)\n",
    "output_name = os.path.basename(version_path).replace(\".txt\", \".json\")\n",
    "output_path = os.path.join(\"../assets/rewards\", output_name)\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(reward, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Reward saved at: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "261c3c54-e491-45e5-8cba-6a97e46a12cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spun' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m attempt = \u001b[32m0\u001b[39m\n\u001b[32m      7\u001b[39m best_reward = \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m best_text = \u001b[43mspun\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_retry_version\u001b[39m(text, label):\n\u001b[32m     11\u001b[39m     path = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m../assets/spun_text/spun_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.txt\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'spun' is not defined"
     ]
    }
   ],
   "source": [
    "# ðŸ“Œ Step 8: RL-Inspired Retry Loop\n",
    "from datetime import datetime\n",
    "\n",
    "MAX_TRIES = 3\n",
    "THRESHOLD = 0.8\n",
    "attempt = 0\n",
    "best_reward = 0\n",
    "best_text = spun\n",
    "\n",
    "def save_retry_version(text, label):\n",
    "    path = f\"../assets/spun_text/spun_{label}.txt\"\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "    print(f\"ðŸ’¾ Saved version: {path}\")\n",
    "    return path\n",
    "\n",
    "while attempt < MAX_TRIES and best_reward < THRESHOLD:\n",
    "    print(f\"\\nðŸ” Iteration {attempt + 1}\")\n",
    "    \n",
    "    # Generate new spin using same writer function\n",
    "    new_spun = spin_text_llama3(original)\n",
    "    reward = calculate_reward(original, new_spun)\n",
    "    final_score = reward[\"final_reward\"]\n",
    "\n",
    "    print(f\"ðŸŽ¯ Reward this iteration: {final_score}\")\n",
    "\n",
    "    # Save if it's better\n",
    "    if final_score > best_reward:\n",
    "        best_reward = final_score\n",
    "        best_text = new_spun\n",
    "        save_retry_version(best_text, f\"best_try{attempt+1}\")\n",
    "        print(f\"âœ… New Best Reward: {best_reward}\")\n",
    "\n",
    "    attempt += 1\n",
    "\n",
    "# ðŸ“Œ Save final best version\n",
    "save_retry_version(best_text, \"final\")\n",
    "print(\"ðŸ“˜ Final version saved with highest reward.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87c5d5d-bcb8-4e7c-b046-4190a49465b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
